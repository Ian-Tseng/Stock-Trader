{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73715a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import in_test_phase\n",
    "from keras import backend \n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from scipy.spatial import distance\n",
    "\n",
    "#from numba import jit, cuda\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "\n",
    "\n",
    "import statistics\n",
    "import gc\n",
    "import datetime\n",
    "import csv\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "#import torch\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import threading\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "\n",
    "\n",
    "def load_model(dir):\n",
    "    \n",
    "    loaded_model = tf.keras.models.load_model(dir)\n",
    "    \n",
    "        \n",
    "    return loaded_model\n",
    "\n",
    "def preprocess(stock_data_dir):\n",
    "    stock_id_list= set()\n",
    "    stock_id_list.add(stock_data_dir)\n",
    "\n",
    "    for stock_history_data_dir in stock_id_list:       \n",
    "        if os.path.exists(stock_history_data_dir):\n",
    "           \n",
    "            title, ext= os.path.splitext(os.path.basename(stock_history_data_dir))\n",
    "            stock_id= title.split('_')[0]\n",
    "            data_list= get_history_data(stock_history_data_dir)\n",
    "            \n",
    "           \n",
    "            \n",
    "            # Standardize\n",
    "    \n",
    "            # open-high-low-close\n",
    "            time_column= [i[0] for i in data_list.copy()]            \n",
    "            open_price_column= [float(i[1]) for i in data_list.copy()]\n",
    "            highest_price_column= [float(i[2]) for i in data_list.copy()]\n",
    "            lowest_price_column= [float(i[3]) for i in data_list.copy()]\n",
    "            close_price_column= [float(i[4]) for i in data_list.copy()]     \n",
    "            \n",
    "            \n",
    "            start_time= time_column[0]\n",
    "            end_time= time_column[-1]\n",
    "            delta_day= convert_2_day_delta(start_time, end_time)\n",
    "      \n",
    "            delta_price= lowest_price_column[-1]- lowest_price_column[0]\n",
    "            slope= delta_price/ delta_day\n",
    "            \n",
    "            \n",
    "            quartile_slope_index_list, quartile_slope_list= get_quartile_slope(data_list, time_column, lowest_price_column) # Return data index list, slope list\n",
    "            \n",
    "            target_quartile_slope_index_list= []\n",
    "            for quartile_slope_index in range(len(quartile_slope_list)):\n",
    "                if quartile_slope_list[quartile_slope_index]> slope:\n",
    "                    target_quartile_slope_index_list.append(quartile_slope_index)\n",
    "            \n",
    "            target_quartile_slope= [ quartile_slope_list[i] for i in target_quartile_slope_index_list]\n",
    "            target_quartile_index_list= [quartile_slope_index_list[i] for i in target_quartile_slope_index_list]\n",
    "           \n",
    "            all_phase_round_range= get_all_phase_round(data_list, target_quartile_slope, target_quartile_index_list)\n",
    "            \n",
    "          \n",
    "            price_delta_column= get_price_delta_column(data_list) # Add price delta column\n",
    "            for index, content in enumerate(data_list):\n",
    "                    content.append(price_delta_column[index])\n",
    "                    \n",
    "            all_period_data_list, target_data_range_begin= get_all_periods_data_to_previous_data_level_with_different_columns(data_list, all_phase_round_range) \n",
    "            \n",
    "            new_data_for_agent_phase_list= []             \n",
    "            print ('Number of periods', np.array(all_period_data_list, dtype= object).shape)\n",
    "            for target_index, target_data_list in enumerate(all_period_data_list):\n",
    "                   # print ('data shape in period', np.array(target_data_list, dtype= object).shape)\n",
    "                    \n",
    "                    sd_level_list, sd_level_tag_column= get_sd_level(target_data_list, price_delta_column) \n",
    "                    data_for_agent_delta_list= target_data_list.copy()\n",
    "            \n",
    "                    for data_index in range(len(data_for_agent_delta_list)):\n",
    "                         \n",
    "                        data_for_agent_delta_list[data_index].append(sd_level_tag_column[data_index]) # Add sd level column\n",
    "                   \n",
    "                    \n",
    "                    data_for_agent_phase_list= data_for_agent_delta_list.copy()       \n",
    "                         \n",
    "                    phase_stage_range_list= get_phase_stage_range(data_for_agent_phase_list)\n",
    "                    minimum_price_column_target_phase_peroid_range= [float(i[2]) for i in data_for_agent_phase_list]  \n",
    "                    phase_stage_column= tag_phase_stage(minimum_price_column_target_phase_peroid_range, phase_stage_range_list)\n",
    " \n",
    "            \n",
    "                    for data_index in range(len(data_for_agent_phase_list)):\n",
    "                        data_for_agent_phase_list[data_index].append(phase_stage_column[data_index]) # Add phase stage column\n",
    "                      \n",
    "                        #data_for_agent_phase_list[data_index].append(phase_stage_range_list) # Add phase stage column list\n",
    "                    \n",
    "                    new_data_for_agent_phase_list+= data_for_agent_phase_list\n",
    "               \n",
    "                  #  agent_name= 'agent_phase_'+ 'train_data_'+ stock_id+ '_period_'+ str(target_index) + '_for_proj'+'.csv'\n",
    "                  #  save_train_data_dir= os.path.join(os.getcwd(), agent_name)\n",
    "                    \n",
    "                  #  save_train_data(save_train_data_dir, data_for_agent_phase_list)\n",
    "            new_data_for_agent_phase_list= new_data_for_agent_phase_list[target_data_range_begin:]\n",
    "                    \n",
    "            agent_name= 'agent_phase_'+ 'train_data_'+ stock_id+'_for_proj'+'.csv'\n",
    "            save_train_data_dir= os.path.join(os.getcwd() , agent_name)\n",
    "                   \n",
    "            save_train_data(save_train_data_dir, new_data_for_agent_phase_list) \n",
    "\n",
    "            \n",
    "def init_training(agent_name, stock_id, refresh_input_data, session_list, total_data_list, num_of_layers, model, memory_state, carry_state, save_model_dir, checkpoint_dir, train_data_dir):\n",
    "    init_train= True\n",
    "    refresh_input_data_in_training= False\n",
    "    init_next_layer= False\n",
    "    save_model_dir=  save_model_dir\n",
    "    checkpoint_dir= checkpoint_dir \n",
    "\n",
    "    state= (memory_state, carry_state)\n",
    "    layer_count= 0\n",
    "    session_index= 1\n",
    "    num_of_layers= num_of_layers\n",
    "    train_times_count= 0\n",
    "    callback_stop_count= 0\n",
    "    callback_stop_val= 5\n",
    "    best_result_count_2_callback_stop_val= 5\n",
    "    predict_data_list= None\n",
    "\n",
    "    model_list= []   \n",
    "    pred_result_list= []\n",
    "    \n",
    "    train_data_list= session_list[session_index][0]\n",
    "    validation_data_list= session_list[session_index][1]\n",
    "        \n",
    "    train_input_data_list, train_input_data_index_list, validation_input_data_list, validation_data_input_index_list, output_train_data_index_list, output_val_data_index_list, _, _= get_input_data_for_other_input(agent_name, train_data_list, validation_data_list, total_data_list, predict_data_list) # Input data\n",
    "    output_value_list= get_expected_output_data(agent_name, output_train_data_index_list)  # Output expeceted data    \n",
    "        \n",
    "   \n",
    "    save_train_input_data_dir= os.path.join(save_model_dir, 'train_input_data')\n",
    "    save_validation_input_data_dir= os.path.join(save_model_dir, 'validation_input_data')\n",
    "\n",
    "    \n",
    "    if not os.path.exists(save_train_input_data_dir+ '.npy') or refresh_input_data:\n",
    "       \n",
    "         \n",
    "        np.save(save_train_input_data_dir, np.array(train_input_data_list))\n",
    "\n",
    "       # save_preprocesss_input_data(save_train_input_data_dir, new_train_input_data_list)\n",
    "    \n",
    "     \n",
    "     \n",
    "        np.save(save_validation_input_data_dir, np.array(validation_input_data_list))\n",
    "       # save_preprocesss_input_data(save_validation_input_data_dir, new_val_input_data_list)\n",
    "     \n",
    "  \n",
    "    new_train_input_data_list= get_preprocesss_input_data(save_train_input_data_dir)\n",
    "    new_validation_input_data_list= get_preprocesss_input_data(save_validation_input_data_dir)\n",
    "\n",
    "    if model:\n",
    "        model_input = model.input\n",
    "\n",
    "        if not model_input[0, :, :].shape== np.array(new_train_input_data_list)[0, :, :].shape:\n",
    "            model= False   \n",
    "        else:   \n",
    "\n",
    "            current_result= get_current_result(session_list, session_index, total_data_list, predict_data_list, agent_name, save_model_dir)\n",
    "            pred_result_list.append(current_result)\n",
    "\n",
    "\n",
    "    while init_train:\n",
    "        gc.enable()\n",
    "        gc.collect()\n",
    "        print ('pred_result_list', pred_result_list)\n",
    "        print ('length of train data', len(train_data_list))\n",
    "        print ('length of validation data', len(validation_data_list))\n",
    "        \n",
    "\n",
    "        # Input data\n",
    "\n",
    "        input_data_list_arr= new_train_input_data_list\n",
    "        output_value_list_arr= np.array(output_value_list)  # Output data\n",
    "        \n",
    "        \n",
    "        batch_size, time_steps, features= input_data_list_arr.shape\n",
    "        print ('batch', batch_size, 'time_steps', time_steps, 'feature', features)\n",
    "\n",
    "        if refresh_input_data_in_training:\n",
    "            session_list, total_data_list= get_cross_train_rule_data_and_validation_data(train_data_dir)\n",
    "            train_data_list= session_list[session_index][0]\n",
    "            validation_data_list= session_list[session_index][1]\n",
    "        \n",
    "            train_input_data_list, train_input_data_index_list, validation_input_data_list, validation_data_input_index_list, output_train_data_index_list, output_val_data_index_list, _, _= get_input_data_for_other_input(agent_name, train_data_list, validation_data_list, total_data_list, predict_data_list) # Input data\n",
    "            output_value_list= get_expected_output_data(agent_name, output_train_data_index_list)  # Output expeceted data    \n",
    "        \n",
    "   \n",
    "            save_train_input_data_dir= os.path.join(save_model_dir, 'train_input_data')\n",
    "            save_validation_input_data_dir= os.path.join(save_model_dir, 'validation_input_data')\n",
    "         \n",
    "            np.save(save_train_input_data_dir, np.array(train_input_data_list))    \n",
    "            np.save(save_validation_input_data_dir, np.array(validation_input_data_list))      \n",
    "  \n",
    "            new_train_input_data_list= get_preprocesss_input_data(save_train_input_data_dir)\n",
    "            new_validation_input_data_list= get_preprocesss_input_data(save_validation_input_data_dir)\n",
    "            refresh_input_data_in_training= False\n",
    "\n",
    "\n",
    "\n",
    "        if model:\n",
    "            model= dropout_non_connect(model)\n",
    "    \n",
    "\n",
    "        if not model:      \n",
    "            #####  Build model  #####   \n",
    "            input_layer= on_input_layer(input_data_list_arr, batch_size, time_steps, features)\n",
    "\n",
    "            layer= input_layer\n",
    "            init_next_layer= True\n",
    "            while init_next_layer:\n",
    "                if layer_count== num_of_layers:\n",
    "                    break\n",
    "                \n",
    "                new_layer, memory_state, carry_state= on_next_layer(layer, layer_count, num_of_layers, state)    \n",
    "\n",
    "                state= (memory_state, carry_state)\n",
    "                layer= new_layer\n",
    "                layer_count+= 1\n",
    "                \n",
    "                \n",
    "            \n",
    "            output_layer= on_output_layer(output_value_list_arr, layer, time_steps, features) \n",
    "\n",
    "            model = tf.keras.Model(inputs= input_layer, outputs= output_layer, name= agent_name+ '_'+ stock_id) # Multi\n",
    "   \n",
    "\n",
    "\n",
    "        #### Batch  training\n",
    "        tf.keras.backend.clear_session()\n",
    "        history= train_model(input_data_list_arr, output_value_list_arr, model, checkpoint_dir, batch_size)\n",
    "\n",
    "        train_times_count+= 1\n",
    "\n",
    "\n",
    "        val_accuracy_list= history.history['val_accuracy']\n",
    "        accuracy_list= history.history['accuracy']\n",
    "\n",
    "        init_pred= False\n",
    "        if train_times_count% 5== 0 or init_pred:\n",
    "            if max(np.array(accuracy_list))>=0.9:\n",
    "                tf.keras.backend.clear_session()\n",
    "                accuracy= init_predict_on_training(model, agent_name, new_validation_input_data_list, validation_data_input_index_list, validation_data_list, output_val_data_index_list) \n",
    "                print ('Acc :', accuracy)\n",
    "                pred_result_list.append(accuracy)\n",
    "                best_result= max(pred_result_list)\n",
    "                refresh_input_data_in_training= True\n",
    "                if len(pred_result_list)> 1 and accuracy== best_result:\n",
    "                    save_model(model, save_model_dir)\n",
    "                    print ('Save the best result :', accuracy)\n",
    "                    callback_stop_count= 0\n",
    "\n",
    "                    unique, counts = np.unique(pred_result_list, return_counts=True)    # Stop training when best results are same.\n",
    "                    best_result_count_2_callback_stop_count= int(dict(zip(unique, counts))[accuracy])\n",
    "                    if best_result_count_2_callback_stop_count== best_result_count_2_callback_stop_val:\n",
    "                        if max(pred_result_list)< 0.6: # Low acc list\n",
    "                            low_acc_list.append([stock_id, max(pred_result_list)])\n",
    "                        init_train= False                  \n",
    "                        break\n",
    "\n",
    "\n",
    "                callback_stop_count+= 1\n",
    "            \n",
    "            if callback_stop_count== callback_stop_val: # Stop training when no more best result\n",
    "                if max(pred_result_list)< 0.6: # Low acc list\n",
    "                    low_acc_list.append([stock_id, max(pred_result_list)])\n",
    "                init_train= False\n",
    "                break\n",
    "\n",
    "        if train_times_count% 20== 0:\n",
    "            if not max(np.array(accuracy_list))>=0.9:\n",
    "                save_model(model, save_model_dir)\n",
    "          \n",
    "      \n",
    "    return model, (memory_state, carry_state)\n",
    "\n",
    "def get_history_data(path):\n",
    "    history_data_list= []\n",
    "    reference_data_list= []\n",
    "    target_data_list= []\n",
    "    reference_data_dir= os.path.join(os.getcwd(), '0050_reference_data.csv')\n",
    "    \n",
    "    with open(path, 'r', newline='', encoding='utf-8-sig') as csvfile:                       \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        for row in reader:\n",
    "            for col_index, col in enumerate(row):\n",
    "                if col_index!= 0:\n",
    "                    if col== 'null':\n",
    "                        continue\n",
    "                    \n",
    "                    col= float(col)\n",
    "                \n",
    "            history_data_list.append(row)\n",
    "            \n",
    "    target_data_begin= history_data_list[0]\n",
    "    target_data_end= history_data_list[-1]   \n",
    "    target_data_begin_index= None\n",
    "    target_data_end_index= None\n",
    "    \n",
    "    with open(reference_data_dir, 'r', newline='', encoding='utf-8-sig') as csvfile:                       \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        data_list= [i for i in reader]\n",
    "        for row in data_list[1:]:\n",
    "            \n",
    "            for col_index, col in enumerate(row):\n",
    "                if col_index!= 0:\n",
    "                    if col== 'null':\n",
    "                        continue\n",
    "                    col= float(col)\n",
    "     \n",
    "            reference_data_list.append(row)\n",
    "        \n",
    "        for reference_data_index, reference_data in enumerate(reference_data_list):\n",
    "            if 'null' in reference_data:\n",
    "                continue\n",
    "                \n",
    "            if [float(i) for i in target_data_begin]==[float(i) for i in reference_data[1: 5]]:\n",
    "               \n",
    "                target_data_begin_index= reference_data_index- 10\n",
    "            if  [float(i) for i in target_data_end]==[float(i) for i in reference_data[1: 5]]:\n",
    "               \n",
    "                target_data_end_index= reference_data_index\n",
    "        \n",
    "    if target_data_begin_index!= None and target_data_end_index!= None:\n",
    "        target_data_list =  reference_data_list[target_data_begin_index: target_data_end_index]\n",
    "    \n",
    "    return target_data_list\n",
    "\n",
    "\n",
    "def get_cross_train_rule_data_and_validation_data(dir):\n",
    "    total_data_list= []\n",
    " \n",
    "    session_list= []\n",
    "    with open(dir, 'r+', newline='') as csvfile:    \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        total_data_list= [i for i in reader]\n",
    "        num_of_total_data= len(total_data_list)\n",
    "        \n",
    "        num_of_val_data= int(num_of_total_data* 1/ 10)   # val rate= 0.1\n",
    "        num_of_train_data= int(num_of_total_data- num_of_val_data)\n",
    "        session_num= 2    # cross validation\n",
    "        bottom_boundary= 15\n",
    "\n",
    "        for session in range(session_num):\n",
    "            total_data_list_copy= total_data_list[bottom_boundary:].copy()\n",
    "            validation_data_list= []  \n",
    "      \n",
    "            for num in range(num_of_val_data):\n",
    "                    rand_sel= random.choice(total_data_list_copy)\n",
    "                    total_data_list_copy.remove(rand_sel)  \n",
    "\n",
    "                    validation_data_list.append(rand_sel)\n",
    "            train_rule_data_list= total_data_list_copy\n",
    "            session_list.append([train_rule_data_list, validation_data_list])\n",
    "    \n",
    "    return session_list, total_data_list\n",
    "\n",
    "\n",
    "def save_train_and_val_data(dir, data):\n",
    "    with open(dir, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def get_train_and_val_data(dir):\n",
    "    with open(dir, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        data_list= []\n",
    "        for sub_content in reader:\n",
    "            data_list.append(sub_content)\n",
    "      \n",
    "    return data_list\n",
    "\n",
    "def get_predict_next_day_data(dir):\n",
    "    with open(dir, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        data_list= []\n",
    "        for sub_content in reader:\n",
    "            data_list.append(sub_content)\n",
    "      \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def get_input_data_for_other_input(agent_name, train_data_list, validation_data_list, total_data_list, predict_data_list):\n",
    "    train_input_data_list= [] \n",
    "    train_input_data_index_list= []\n",
    "    validation_input_data_list= []\n",
    "    validation_input_data_index_list= []\n",
    "    predict_next_day_input_data_list= []\n",
    "    predict_next_day_input_data_index_list= []\n",
    "    output_train_data_index_list= []  # Output data index = [stock id, 日期 , specific agent output]\n",
    "    output_val_data_index_list= []\n",
    "    plus_representation_coding= 1\n",
    "    substraction_representation_coding= 0\n",
    "    total_date_data= [i[0] for i in total_data_list]\n",
    "    data_range= 7\n",
    "    stock_id= '0050'\n",
    "    for train_data in train_data_list[data_range:]:\n",
    "        if agent_name== 'agent_phase':               # Agent phase train data\n",
    "            target_data_range= get_previous_data_range(train_data, data_range, total_data_list, total_date_data) # Use previous days data to predict next day\n",
    "           # target_data_range= get_previous_data_range_use_data_len(train_data, data_range, total_data_list)\n",
    "          \n",
    "            data_in_day_level= []\n",
    "            for target_data in target_data_range:\n",
    "                if len(target_data)< 10:\n",
    "                     continue\n",
    "                oepn_price= target_data[8]     # Open price level\n",
    "                oepn_price_list= ast.literal_eval(oepn_price)\n",
    "\n",
    "                \n",
    "                highest_price= target_data[9] # Highest price level\n",
    "                highest_price_list= ast.literal_eval(highest_price)\n",
    "\n",
    "\n",
    "                lowest_price= target_data[10]            # Lowest price level\n",
    "                lowest_price_list= ast.literal_eval(lowest_price)\n",
    "                   \n",
    "                close_price= target_data[11]           # Close price level\n",
    "                close_price_list= ast.literal_eval(close_price)\n",
    "\n",
    "\n",
    "                delta= [float(target_data[7])]  # Delta level\n",
    "                  \n",
    "               # feature_list= [0]+ oepn_price_list+ [1]+ highest_price_list+ [2]+ lowest_price_list+ [3]+ close_price_list \n",
    "                feature_list= [0]+ oepn_price_list+ [1]+ close_price_list      \n",
    "                \n",
    "                data_in_day_level.append(feature_list)\n",
    "            train_input_data_list.append(data_in_day_level)\n",
    "            train_input_data_index_list.append([stock_id, train_data[0]])\n",
    "            output_train_data_index_list.append([stock_id, train_data[0], train_data[14]])\n",
    "\n",
    "    \n",
    "    for validation_data in validation_data_list:\n",
    "        if agent_name== 'agent_phase':   # Agent phase validation data\n",
    "            target_data_range= get_previous_data_range(validation_data, data_range, total_data_list, total_date_data) # Use previous days data to predict next day\n",
    "          #  target_data_range= get_previous_data_range_use_data_len(train_data, data_range, total_data_list)\n",
    "            data_in_day_level= []\n",
    "            for target_data in target_data_range:\n",
    "                if len(target_data)< 10:\n",
    "                     continue\n",
    "                oepn_price= target_data[8]     # Open price level\n",
    "                oepn_price_list= ast.literal_eval(oepn_price)\n",
    "\n",
    "                \n",
    "                highest_price= target_data[9] # Highest price level\n",
    "                highest_price_list= ast.literal_eval(highest_price)\n",
    "\n",
    "\n",
    "                lowest_price= target_data[10]            # Lowest price level\n",
    "                lowest_price_list= ast.literal_eval(lowest_price)\n",
    "                   \n",
    "                close_price= target_data[11]           # Close price level\n",
    "                close_price_list= ast.literal_eval(close_price)\n",
    "\n",
    "\n",
    "                delta= [float(target_data[7])]  # Delta level\n",
    "                           \n",
    "                #feature_list= [0]+ oepn_price_list+ [1]+ highest_price_list+ [2]+ lowest_price_list+ [3]+ close_price_list     \n",
    "                feature_list= [0]+ oepn_price_list+ [1]+ close_price_list\n",
    "                data_in_day_level.append(feature_list)\n",
    "            validation_input_data_list.append(data_in_day_level)\n",
    "            validation_input_data_index_list.append([stock_id, validation_data[0]])\n",
    "            output_val_data_index_list.append([stock_id, validation_data[0], validation_data[14]]) # Output index = [stock id, 日期 , phase stage]\n",
    "\n",
    "    \n",
    "    if predict_data_list!= None:\n",
    "    \n",
    "        total_date_data= [i[0] for i in predict_data_list]\n",
    "        predict_next_day= predict_data_list[-1]  # The validation data \n",
    "        predict_next_date_data_range= get_previous_data_range(predict_next_day, data_range, predict_data_list, total_date_data)\n",
    "\n",
    "        data_in_day_level= []\n",
    "        for target_data in predict_next_date_data_range:\n",
    "                if len(target_data)< 10:\n",
    "                     continue\n",
    "                oepn_price= target_data[8]     # Open price level\n",
    "                oepn_price_list= ast.literal_eval(oepn_price)\n",
    "\n",
    "                \n",
    "                highest_price= target_data[9] # Highest price level\n",
    "                highest_price_list= ast.literal_eval(highest_price)\n",
    "\n",
    "\n",
    "                lowest_price= target_data[10]            # Lowest price level\n",
    "                lowest_price_list= ast.literal_eval(lowest_price)\n",
    "                   \n",
    "                close_price= target_data[11]           # Close price level\n",
    "                close_price_list= ast.literal_eval(close_price)\n",
    "\n",
    "\n",
    "                delta= [float(target_data[7])]  # Delta level\n",
    "                           \n",
    "                #feature_list= [0]+ oepn_price_list+ [1]+ highest_price_list+ [2]+ lowest_price_list+ [3]+ close_price_list      \n",
    "                feature_list= [0]+ oepn_price_list+ [1]+ close_price_list\n",
    "                data_in_day_level.append(feature_list)\n",
    "\n",
    "        predict_next_day_input_data_list.append(data_in_day_level)\n",
    "        predict_next_day_input_data_index_list.append([stock_id, train_data_list[-1][0]])\n",
    "    return train_input_data_list, train_input_data_index_list, validation_input_data_list, validation_input_data_index_list, output_train_data_index_list, output_val_data_index_list, predict_next_day_input_data_list, predict_next_day_input_data_index_list\n",
    "    \n",
    "\n",
    "def get_predict_previous_data_range(target_predict_data, data_range, total_data_list, total_date_data):\n",
    "    month_eng_2_index_list= ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    current_data_date= target_predict_data\n",
    "   \n",
    "    current_y, current_m, current_d= int(current_data_date[: -4])+1911, int(current_data_date[-4: -2]), int(current_data_date[-2:])  # Get target date range\n",
    "    \n",
    "    target_data_range= []\n",
    "    date_count= 1\n",
    "    while len(target_data_range)< data_range:\n",
    "                current_date_2_sec= datetime.datetime(current_y, current_m, current_d).timestamp()\n",
    "                previous_date_sce= int(current_date_2_sec- 3600* 24* date_count)\n",
    "                previous_date= time.ctime(previous_date_sce)\n",
    "\n",
    "                previous_date_y, previous_date_m, previous_date_d= int(previous_date[-4:])- 1911, int(month_eng_2_index_list.index(previous_date[4:7]))+ 1, int(previous_date[8:11])\n",
    "                if previous_date_m< 10:\n",
    "                    previous_date_m= str(0)+ str(previous_date_m)\n",
    "                else:\n",
    "                    previous_date_m= str(previous_date_m)   \n",
    "                if previous_date_d< 10:\n",
    "                    previous_date_d= str(0)+ str(previous_date_d)\n",
    "                else:\n",
    "                    previous_date_d= str(previous_date_d)              \n",
    "                target_previous_date= int(str(previous_date_y)+ previous_date_m+ str(previous_date_d))\n",
    "                if target_previous_date in total_date_data:\n",
    "                    \n",
    "                    target_previous_data_index= total_date_data.index(target_previous_date)\n",
    "                    target_previous_data= total_data_list[target_previous_data_index]\n",
    "                    target_data_range.append(target_previous_data)\n",
    "                  \n",
    "                date_count+= 1\n",
    "\n",
    "    return target_data_range\n",
    "\n",
    "\n",
    "def get_previous_data_range(target_data, data_range, total_data_list, total_date_data):\n",
    "    month_eng_2_index_list= ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    current_data_date= target_data[0]\n",
    "   \n",
    "    current_y, current_m, current_d= int(current_data_date.split('-')[0]), int(current_data_date.split('-')[1]), int(current_data_date.split('-')[2])  # Get target date range\n",
    "    \n",
    "    target_data_range= []\n",
    "    date_count= 1\n",
    " \n",
    "    while len(target_data_range)< data_range:\n",
    "                current_date_2_sec= datetime.datetime(current_y, current_m, current_d).timestamp()\n",
    "                previous_date_sce= int(current_date_2_sec- 3600* 24* date_count)\n",
    "                previous_date= time.ctime(previous_date_sce)\n",
    "               \n",
    "                previous_date_y, previous_date_m, previous_date_d= int(previous_date[-4:]), int(month_eng_2_index_list.index(previous_date[4:7]))+ 1, int(previous_date[8:11])\n",
    "              \n",
    "                if previous_date_m< 10:\n",
    "                    previous_date_m= str(0)+ str(previous_date_m)\n",
    "                else:\n",
    "                    previous_date_m= str(previous_date_m)   \n",
    "                if previous_date_d< 10:\n",
    "                    previous_date_d= str(0)+ str(previous_date_d)\n",
    "                else:\n",
    "                    previous_date_d= str(previous_date_d)\n",
    "                                  \n",
    "                target_previous_date= str(previous_date_y)+'-'+ previous_date_m+'-'+ str(previous_date_d)\n",
    "                if target_previous_date in total_date_data:\n",
    "                    \n",
    "                    target_previous_data_index= total_date_data.index(target_previous_date)\n",
    "                    target_previous_data= total_data_list[target_previous_data_index]\n",
    "                    target_data_range.append(target_previous_data)\n",
    "                  \n",
    "                date_count+= 1\n",
    "\n",
    "    return target_data_range\n",
    "\n",
    "\n",
    "\n",
    "def get_expected_output_data(agent_name, target_index_data_list):\n",
    "    # Delta: 20 level\n",
    "    # Phase: 8 stage\n",
    "    \n",
    "    all_output_val_list= []\n",
    "    target_data_date_list= [i[1] for i in target_index_data_list]\n",
    "    target_data_value_list= [i[2] for i in target_index_data_list]\n",
    "\n",
    "\n",
    "  \n",
    "    if agent_name== 'agent_delta':\n",
    "        output_val_list = list(np.zeros(20))\n",
    "    else:\n",
    "        output_val_list= list(np.zeros(8))\n",
    "   \n",
    "    for target_data_value in target_data_value_list:\n",
    "   \n",
    "        new_output_val_list= output_val_list.copy()\n",
    "        new_output_val_list[int(target_data_value)]= 1\n",
    "        all_output_val_list.append(new_output_val_list)\n",
    "\n",
    "        \n",
    "    return all_output_val_list\n",
    "        \n",
    "\n",
    "def get_current_result(session_list, session_index, total_data_list, predict_data_list, agent_name, save_model_dir):\n",
    "\n",
    "    save_validation_input_data_dir= os.path.join(save_model_dir, 'validation_input_data')\n",
    "    new_validation_input_data_list= get_preprocesss_input_data(save_validation_input_data_dir)\n",
    "\n",
    "    train_data_list= session_list[session_index][0]\n",
    "    validation_data_list= session_list[session_index][1]\n",
    "        \n",
    "    train_input_data_list, train_input_data_index_list, validation_input_data_list, validation_data_input_index_list, output_train_data_index_list, output_vali_data_index_list, _, _= get_input_data_for_other_input(agent_name, train_data_list, validation_data_list, total_data_list, predict_data_list)\n",
    "\n",
    "    model= load_model(save_model_dir)\n",
    "    acc= init_predict_on_training(model, agent_name, new_validation_input_data_list, validation_data_input_index_list, validation_data_list, output_vali_data_index_list)\n",
    "\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def dropout_non_connect(model):\n",
    "    target_layer_id_list= [1, 2, 3, 4, 5, 6]\n",
    "    for layer_id in target_layer_id_list:\n",
    "\n",
    "                target_lstm_layer_weights = model.layers[layer_id].get_weights()[0]\n",
    "                target_lstm_layer_weights_shape= target_lstm_layer_weights.shape\n",
    "                target_lstm_layer_weights_size= target_lstm_layer_weights.size\n",
    "                new_target_lstm_layer_weights= target_lstm_layer_weights.reshape(target_lstm_layer_weights_size)\n",
    "                \n",
    "                for weight in new_target_lstm_layer_weights:\n",
    "                    if weight< 0:\n",
    "                        weight= 0\n",
    "                  \n",
    "                new_target_lstm_layer_weights= new_target_lstm_layer_weights.reshape(target_lstm_layer_weights_shape)\n",
    "                new_param=  model.layers[layer_id].get_weights()\n",
    "                new_param[0]= new_target_lstm_layer_weights           \n",
    "                model.layers[layer_id].set_weights(new_param)\n",
    "                \n",
    "    return model\n",
    "    \n",
    "  \n",
    "\n",
    "     \n",
    "\n",
    "def on_preprocessing_layer_multi_hot(input_data_list):\n",
    "    layer= tf.keras.layers.CategoryEncoding(num_tokens= 60, output_mode=\"multi_hot\")\n",
    "    new_input_data= layer(input_data_list).numpy()\n",
    "    #new_input_data_list= tf.keras.layers.Embedding( # Input_dim: Integer. i.e. maximum integer index + 1.\n",
    "    #  1000,\n",
    "    #  len(input_data_list),\n",
    "    #  embeddings_initializer=\"uniform\",\n",
    "    #  embeddings_regularizer=None,\n",
    "    #  activity_regularizer=None,\n",
    "    #  embeddings_constraint=None,\n",
    "    #  mask_zero=False,\n",
    "    #  input_length=None,\n",
    "    #)(input_data_list)\n",
    "    return new_input_data\n",
    "\n",
    "\n",
    "\n",
    "def on_input_layer(input_data_list, batch_size, time_steps, features):\n",
    "   # input= [samples, time_steps, features] [Number of datas, The length of each data, Each element of the data is a vector of n features]\n",
    "   # Samples - Number of datas\n",
    "   # Time steps -   The length of each data\n",
    "   # Features - Each element of the data is a vector of n features\n",
    "    input_layer = tf.keras.Input(shape=(time_steps, features), name='input_layer') # shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors.\n",
    "\n",
    "    return input_layer\n",
    "  \n",
    "def on_next_layer(layer, layer_count, num_of_layers, state):\n",
    "    if state[0]== None:\n",
    "        state= None\n",
    "    if layer_count== num_of_layers-1:\n",
    "        dropout= 0\n",
    "    else:\n",
    "        dropout= 0\n",
    "    number_of_units= 128 \n",
    "\n",
    "    _, time_steps, features= layer.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    new_layer, memory_state, carry_state= tf.keras.layers.LSTM(units = number_of_units, input_shape= (time_steps, features), name= 'hidden_layer_'+ str(layer_count), return_sequences=True, stateful= False, return_state= True\n",
    "                                  , time_major= False, activation=\"tanh\", recurrent_activation=\"sigmoid\", unit_forget_bias=True, \n",
    "                                  kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", dropout= dropout, \n",
    "                                  use_bias=True)(layer, state)\n",
    "\n",
    "    \n",
    "   # if not layer_count== num_of_layers- 2:\n",
    "   #     new_layer = new_layer[0, :, :]\n",
    "   #     new_layer= tf.keras.layers.RepeatVector(time_steps)(new_layer)\n",
    "        \n",
    "\n",
    "    if layer_count== num_of_layers-1:\n",
    "        new_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(features))(new_layer) # TimeDistributed applies the same instance of previous layer to each of the timestamps, the same set of weights are used at each timestamp.\n",
    "\n",
    "    return new_layer, memory_state, carry_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def on_output_layer(output_value_list, layer, time_steps, features):\n",
    "    outputs= tf.keras.layers.Flatten(data_format=None)(layer)\n",
    "    outputs= tf.keras.layers.Dropout(0.2) (outputs, training=True) # 0.25\n",
    " \n",
    "    outputs= tf.keras.layers.Dense(units = len(output_value_list[0]), activation='relu', name='output_dense_layer_0', use_bias=True)(outputs)\n",
    "    outputs= tf.keras.layers.Dense(units = len(output_value_list[0]), activation='sigmoid', name='output_dense_layer_1', use_bias=True)(outputs)\n",
    "    print ('outputs shape', outputs.shape)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def train_model(input_data_list, data_expected_output, model, checkpoint_dir, batch_size):\n",
    "    optimizer_Adam= tf.keras.optimizers.Adam(learning_rate= 1e-3,\n",
    "                                              beta_1=0.9,\n",
    "                                              beta_2=0.999,\n",
    "                                              epsilon=1e-07,\n",
    "    )\n",
    "\n",
    "    rms_prop= tf.keras.optimizers.RMSprop(\n",
    "        learning_rate=1e-3,\n",
    "        rho=0.9,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-07,\n",
    "        centered=False,\n",
    "        name=\"RMSprop\",\n",
    "    )\n",
    "\n",
    "    ada_delta= tf.keras.optimizers.Adadelta(\n",
    "        learning_rate=1e-3, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    "    )\n",
    "\n",
    "    sgd= tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "    binary_crossentropy= tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits= True,\n",
    "                label_smoothing=0.0,\n",
    "                axis=-1,\n",
    "                reduction=\"auto\",\n",
    "                name=\"binary_crossentropy\",\n",
    "    )\n",
    "\n",
    "    categorical_crossentropy= tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=False,\n",
    "        label_smoothing=0.0,\n",
    "        axis=-1,\n",
    "      #  reduction=\"auto\",\n",
    "        name=\"categorical_crossentropy\",\n",
    "    ) \n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\", # val_loss\n",
    "        # \"No longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"No longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode= 'auto'\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir,\n",
    "        monitor='val_accuracy',\n",
    "        mode='auto',\n",
    "        save_weights_only= True,\n",
    "        save_best_only=True\n",
    "        )    \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(#optimizer= sgd,\n",
    "              optimizer= optimizer_Adam, \n",
    "              # Loss function to minimize\n",
    "              loss= tf.keras.losses.CategoricalCrossentropy(),\n",
    "              # List of metrics to monitor    \n",
    "              metrics=['accuracy', 'mse']\n",
    "            )\n",
    "\n",
    "    print ('input_data_list shape', input_data_list.shape)\n",
    "    print ('data_expected_output shape', data_expected_output.shape)\n",
    "  \n",
    " \n",
    "   # print('# Fit model on training data')\n",
    "   # model.summary()\n",
    "    batch_size= 32\n",
    "    number_of_epochs= 100\n",
    "\n",
    "    history = model.fit(input_data_list, \n",
    "            data_expected_output, \n",
    "        batch_size= batch_size, \n",
    "        epochs= number_of_epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks= callbacks,\n",
    "                )\n",
    "  \n",
    "   # plt.plot(history.history['accuracy'])\n",
    "   # plt.plot(history.history['val_accuracy'])\n",
    "   # plt.title('Model accuracy')\n",
    "   # plt.ylabel('Accuracy')\n",
    "   # plt.xlabel('Epoch')\n",
    "   # plt.legend(['Train', 'Test'], loc='upper left')\n",
    "   # plt.show()\n",
    "\n",
    " # results = model.evaluate(validation_input_data_list, validation_expected_output, batch_size=128)\n",
    " # print(\"test loss, test acc:\", results)\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "def save_model(Model, dir):\n",
    "    Model.save(\n",
    "        dir,\n",
    "      overwrite=True,\n",
    "      include_optimizer=True,\n",
    "      save_format=None,\n",
    "      signatures=None,\n",
    "      options=None,\n",
    "      save_traces=True,\n",
    "    )\n",
    "\n",
    "def load_model(dir):\n",
    "    try:\n",
    "        loaded_model = tf.keras.models.load_model(dir)\n",
    "    except:\n",
    "        loaded_model= False\n",
    "    return loaded_model\n",
    "\n",
    "def model_predict(predict_data, model):\n",
    "  \n",
    "    prediction= model.predict(\n",
    "        predict_data,\n",
    "        batch_size=None,\n",
    "        verbose=\"auto\",\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "      )\n",
    "    return prediction\n",
    "\n",
    "def init_predict_on_training(model, agent_name, new_predict_input_data_list, predict_input_data_index, predict_data_list, output_data_index_list): \n",
    "    \n",
    "    predict_input_data_list_arr= new_predict_input_data_list\n",
    "    expected_output_value_list= get_expected_output_data(agent_name, output_data_index_list)\n",
    "  \n",
    "    tp_count= 0\n",
    "    for predict_data, predict_data_index, expected_output_value_index in zip(predict_input_data_list_arr, predict_input_data_index, expected_output_value_list):\n",
    "        detail= None\n",
    "        SE= 0\n",
    "        time_steps, features= predict_data.shape\n",
    "        new_predict_data= predict_data.reshape(1, time_steps, features)\n",
    "        prediction= model_predict(new_predict_data, model)\n",
    " \n",
    "        max_val= np.max(prediction)\n",
    "        test, value_index= np.where(prediction== max_val)\n",
    "        if len(value_index)!= 1:\n",
    "            print (predict_data_index, \"can't predict data\")\n",
    "            continue\n",
    "        taget_expected_output_value_index= expected_output_value_index.index(1)\n",
    "        if value_index== taget_expected_output_value_index:\n",
    "            tp_count+= 1\n",
    "    accuracy=  tp_count/ len(predict_input_data_list_arr)      \n",
    "    # print ('predict_score', predict_score )\n",
    "    print ('Acuracy', accuracy)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def model_predict(predict_data, model):\n",
    " \n",
    "\n",
    "    prediction= model.predict(\n",
    "        predict_data,\n",
    "        batch_size=None,\n",
    "        verbose=\"auto\",\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "    )\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def init_predict(model, predict_next_day_data_list, predict_next_day_index_data_list): \n",
    "\n",
    "    predict_next_day_data_list_arr= np.array(predict_next_day_data_list)\n",
    "    tp_count= 0\n",
    "    for predict_data_index, predict_data in zip(predict_next_day_index_data_list, predict_next_day_data_list_arr):\n",
    "        detail= None\n",
    "        SE= 0\n",
    "        time_steps, features= predict_data.shape\n",
    "        new_predict_data= predict_data.reshape(1, time_steps, features)\n",
    "        prediction= model_predict(new_predict_data, model)\n",
    "        \n",
    "        max_val= np.max(prediction)\n",
    "        test, value_index= np.where(prediction== max_val)\n",
    "        if len(value_index)!= 1:\n",
    "            print (predict_data_index[0], \"can't predict data\")\n",
    "            continue\n",
    "        print (value_index)\n",
    "        \n",
    "    return value_index\n",
    "\n",
    "def save_preprocesss_input_data(dir, data_list):\n",
    "    with open(dir, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for data in data_list:\n",
    "            writer.writerow(data)\n",
    "\n",
    "def get_preprocesss_input_data(dir):\n",
    "    data_list= np.load(dir+ '.npy', allow_pickle= True)\n",
    "   \n",
    "    return data_list\n",
    "\n",
    "def set_and_init_predict():\n",
    "    agent_name= 'agent_phase'\n",
    "    stock_id= '0050'\n",
    "    task_name= 'for_proj'\n",
    "    \n",
    "    dir= os.getcwd()\n",
    "    save_model_dir= os.path.join(dir, agent_name+'_'+ stock_id) \n",
    "    checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')\n",
    "\n",
    "    train_data_name= agent_name+ '_train_data_'+ 'training'+ '_'+ task_name+'.csv'\n",
    "    train_data_dir= os.path.join(dir , train_data_name)\n",
    "\n",
    "    model= load_model(save_model_dir)\n",
    "   \n",
    "\n",
    "    next_day_data_name= agent_name+'_test_data_' +'testing'+ '_'+ task_name+ '.csv'\n",
    "    predict_next_day_data_dir= os.path.join(dir, next_day_data_name)\n",
    "    \n",
    "    #testing_data = load_data(args.testing)\n",
    "    predict_next_day_data_list= get_predict_next_day_data(predict_next_day_data_dir)\n",
    "    session_list, total_data_list= get_cross_train_rule_data_and_validation_data(train_data_dir)\n",
    "\n",
    "    session_index= 0\n",
    "    train_data_list= session_list[session_index][0]\n",
    "    validation_data_list= session_list[session_index][1]\n",
    "\n",
    "    train_input_data_list, train_input_data_index_list, validation_input_data_list, validation_data_input_index_list, output_train_data_index_list, output_val_data_index_list,predict_next_day_input_data_list, predict_next_day_input_data_index_list= get_input_data_for_other_input(agent_name, train_data_list, validation_data_list, total_data_list, predict_next_day_data_list) \n",
    "    print (predict_next_day_data_list[-1][0:4])\n",
    "\n",
    "    predict_value= init_predict(model, predict_next_day_input_data_list, predict_next_day_input_data_index_list)\n",
    "    \n",
    "    return predict_value\n",
    "\n",
    "\n",
    "def set_and_init_train():\n",
    "    agent_name= 'agent_phase'\n",
    "    stock_id= '0050'\n",
    "    task_name= 'for_proj'\n",
    "    dir= os.getcwd()\n",
    "    save_model_dir= os.path.join(dir, agent_name+'_'+ stock_id) \n",
    "    checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')\n",
    "\n",
    "    train_data_name= agent_name+ '_train_data_'+ 'training'+ '_'+ task_name+'.csv'\n",
    "    train_data_dir= os.path.join(dir , train_data_name)\n",
    "   \n",
    "\n",
    "    next_day_data_name= agent_name+'_test_data_' +'testing'+ '_'+ task_name+ '.csv'\n",
    "    predict_next_day_data_dir= os.path.join(dir, next_day_data_name)\n",
    "    \n",
    "    #testing_data = load_data(args.testing)\n",
    "    memory_state= None\n",
    "    carry_state= None\n",
    "    \n",
    "    session_list, total_data_list= get_cross_train_rule_data_and_validation_data(train_data_dir)\n",
    "\n",
    "    session_index= 0\n",
    "    train_data_list= session_list[session_index][0]\n",
    "    validation_data_list= session_list[session_index][1]\n",
    "\n",
    "    number_of_layers= 6\n",
    "\n",
    "    if not os.path.exists(save_model_dir):\n",
    "        os.mkdir(save_model_dir)\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    refresh_input_data= True\n",
    "    model= False\n",
    "\n",
    "    model, (memory_state, carry_state)= init_training(agent_name, stock_id, refresh_input_data, session_list, total_data_list, number_of_layers, model, memory_state, carry_state, save_model_dir, checkpoint_dir, train_data_dir)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_outline_history_data_for_test(path):\n",
    "    history_data_list= []\n",
    "    reference_data_list= []\n",
    "    target_data_list= []\n",
    "    reference_data_dir= os.path.join(os.getcwd(), '0050_reference_data.csv')\n",
    "    \n",
    "    with open(path, 'r', newline='', encoding='utf-8-sig') as csvfile:                       \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        for row in reader:\n",
    "            for col_index, col in enumerate(row):\n",
    "                if col_index!= 0:\n",
    "                    if col== 'null':\n",
    "                        continue\n",
    "                    \n",
    "                    col= float(col)\n",
    "                \n",
    "            history_data_list.append(row)\n",
    "            \n",
    "    target_data_begin= history_data_list[0]\n",
    "    target_data_end= history_data_list[-1]   \n",
    "    target_data_begin_index= None\n",
    "    target_data_end_index= None\n",
    "    \n",
    "    with open(reference_data_dir, 'r', newline='', encoding='utf-8-sig') as csvfile:                       \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        data_list= [i for i in reader]\n",
    "        for row in data_list[1:]:\n",
    "            \n",
    "            for col_index, col in enumerate(row):\n",
    "                if col_index!= 0:\n",
    "                    if col== 'null':\n",
    "                        continue\n",
    "                    col= float(col)\n",
    "     \n",
    "            reference_data_list.append(row)\n",
    "        \n",
    "        for reference_data_index, reference_data in enumerate(reference_data_list):\n",
    "            if 'null' in reference_data:\n",
    "                continue\n",
    "                \n",
    "            if [float(i) for i in target_data_begin]==[float(i) for i in reference_data[1: 5]]:               \n",
    "                target_data_begin_index= reference_data_index\n",
    "                \n",
    "            if [float(i) for i in target_data_end]==[float(i) for i in reference_data[1: 5]]:           \n",
    "                target_data_end_index= reference_data_index+ 1\n",
    "        \n",
    "    if target_data_begin_index!= None and target_data_end_index!= None:\n",
    "        target_data_list =  reference_data_list[target_data_begin_index: target_data_end_index]\n",
    "\n",
    "    \n",
    "    return target_data_list\n",
    "\n",
    "def preprocess_for_test_data(test_data_dir, target_row):\n",
    "    title, ext= title, ext= os.path.splitext(os.path.basename(test_data_dir))\n",
    "    stock_id= title.split('_')[0]\n",
    "   # data_list= get_history_data_for_test(test_data_dir, target_row)\n",
    "    data_list= get_target_row_history_data_for_test(test_data_dir, target_row)\n",
    "\n",
    "    # Standardize\n",
    "    \n",
    "    # open-high-low-close\n",
    "    time_column= [i[0] for i in data_list.copy()]            \n",
    "    lowest_price_column= [float(i[3]) for i in data_list.copy()]\n",
    "            \n",
    "            \n",
    "    start_time= time_column[0]\n",
    "    end_time= time_column[-1]\n",
    "    delta_day= convert_2_day_delta(start_time, end_time)\n",
    "      \n",
    "    delta_price= lowest_price_column[-1]- lowest_price_column[0]\n",
    "    slope= delta_price/ delta_day\n",
    "            \n",
    "            \n",
    "    quartile_slope_index_list, quartile_slope_list= get_quartile_slope(data_list, time_column, lowest_price_column) # Return data index list, slope list\n",
    "            \n",
    "    target_quartile_slope_index_list= []\n",
    "    for quartile_slope_index in range(len(quartile_slope_list)):\n",
    "        if quartile_slope_list[quartile_slope_index]> slope:\n",
    "            target_quartile_slope_index_list.append(quartile_slope_index)\n",
    "            \n",
    "    target_quartile_slope= [ quartile_slope_list[i] for i in target_quartile_slope_index_list]\n",
    "    target_quartile_index_list= [quartile_slope_index_list[i] for i in target_quartile_slope_index_list]\n",
    "   \n",
    "    all_phase_round_range= get_all_phase_round(data_list, target_quartile_slope, target_quartile_index_list)\n",
    "            \n",
    "          \n",
    "    price_delta_column= get_price_delta_column(data_list) # Add price delta column\n",
    "    for index, content in enumerate(data_list):\n",
    "                    content.append(price_delta_column[index])\n",
    "                    \n",
    "    data_list= get_period_data_to_previous_data_level_with_different_columns(data_list, all_phase_round_range) \n",
    "   \n",
    "  \n",
    "                    \n",
    "    sd_level_list, sd_level_tag_column= get_sd_level(data_list, price_delta_column) \n",
    "    data_for_agent_delta_list= data_list.copy()\n",
    "            \n",
    "    for data_index in range(len(data_for_agent_delta_list)):\n",
    "                         \n",
    "        data_for_agent_delta_list[data_index].append(sd_level_tag_column[data_index]) # Add sd level column\n",
    "                   \n",
    "                    \n",
    "    data_for_agent_phase_list= data_for_agent_delta_list.copy()       \n",
    "   \n",
    "    phase_stage_range_list= get_phase_stage_range(data_for_agent_phase_list)\n",
    "    minimum_price_column_target_phase_peroid_range= [float(i[2]) for i in data_for_agent_phase_list]  \n",
    "    phase_stage_column= tag_phase_stage(minimum_price_column_target_phase_peroid_range, phase_stage_range_list)\n",
    " \n",
    "            \n",
    "    for data_index in range(len(data_for_agent_phase_list)):\n",
    "        data_for_agent_phase_list[data_index].append(phase_stage_column[data_index]) # Add phase stage column\n",
    "                      \n",
    "                        #data_for_agent_phase_list[data_index].append(phase_stage_range_list) # Add phase stage column list\n",
    "                    \n",
    "    #new_data_for_agent_phase_list+= data_for_agent_phase_list\n",
    "                    \n",
    "               \n",
    "    agent_name= 'agent_phase_'+ 'test_data_'+ stock_id + '_for_proj'+'.csv'\n",
    "    save_train_data_dir= os.path.join(os.getcwd(), agent_name)\n",
    "                    \n",
    "    save_train_data(save_train_data_dir, data_for_agent_phase_list) \n",
    "    \n",
    "    \n",
    "def get_target_row_history_data_for_test(path, target_row):\n",
    "    reference_data_list= []\n",
    "    target_data_list= []\n",
    "    reference_data_dir= os.path.join(os.getcwd(), '0050_reference_data.csv')\n",
    "\n",
    "\n",
    "    target_data_begin_index= None\n",
    "    target_data_end_index= None\n",
    "    \n",
    "    with open(reference_data_dir, 'r', newline='', encoding='utf-8-sig') as csvfile:                       \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        data_list= [i for i in reader]\n",
    "        for row in data_list[1:]:\n",
    "            \n",
    "            for col_index, col in enumerate(row):\n",
    "                if col_index!= 0:\n",
    "                    if col== 'null':\n",
    "                        continue\n",
    "                    col= float(col)\n",
    "     \n",
    "            reference_data_list.append(row)\n",
    "        \n",
    "        for reference_data_index, reference_data in enumerate(reference_data_list):\n",
    "            if 'null' in reference_data:\n",
    "                continue\n",
    "                \n",
    "            if [float(i) for i in target_row[1:5]]==[float(i) for i in reference_data[1: 5]]:\n",
    "               \n",
    "                target_data_begin_index= reference_data_index- 10 # Data range= 7         \n",
    "                target_data_end_index= reference_data_index+ 1\n",
    "        \n",
    "    if target_data_begin_index!= None and target_data_end_index!= None:\n",
    "        target_data_list =  reference_data_list[target_data_begin_index: target_data_end_index]\n",
    "\n",
    "    \n",
    "    return target_data_list\n",
    "\n",
    "def convert_2_day_delta(a_time, b_time):\n",
    "   \n",
    "    a_time_y= int(a_time.split('-')[0])\n",
    "    b_time_y= int(b_time.split('-')[0])\n",
    "    a_time_m= int(a_time.split('-')[1])\n",
    "    b_time_m= int(b_time.split('-')[1])\n",
    "    a_time_d= int(a_time.split('-')[2])\n",
    "    b_time_d= int(b_time.split('-')[2])\n",
    "    \n",
    "    a_time_datetime = datetime.datetime(a_time_y, a_time_m, a_time_d)\n",
    "    b_time_datetime= datetime.datetime(b_time_y, b_time_m, b_time_d)  \n",
    " \n",
    "    delta_seconds = (b_time_datetime- a_time_datetime).total_seconds()\n",
    "\n",
    "    delta_days= delta_seconds/ 3600\n",
    "    return delta_days\n",
    "\n",
    "def get_quartile_slope(data_list, time_column, minimum_price_column):\n",
    "    quartile_slope_index_list= []\n",
    "    quartile_slope_content_list= []\n",
    "    target_time_index_list= []\n",
    "    target_time_content_list= []\n",
    "    \n",
    "    \n",
    "    for time_index, time in enumerate(time_column):\n",
    "        \n",
    "        time_y= int(time.split('-')[0])\n",
    "        time_m= int(time.split('-')[1])\n",
    "        time_d= int(time.split('-')[2])\n",
    "        target_time_m= [1, 2, 3, 4, 5 , 6, 7, 8, 9, 10, 11, 12]     \n",
    "\n",
    "        if time_m in target_time_m:\n",
    "            begin_time_d= int(min([i[:].split('-')[2] for i in time_column if int(i[:].split('-')[1]) == time_m]))          # Get begin day in moth\n",
    "            if time_d== begin_time_d:\n",
    "                \n",
    "                target_time_index_list.append(time_index)\n",
    "                target_time_content_list.append(time)        \n",
    "\n",
    "    target_minimum_price_list= []\n",
    "    for i in target_time_index_list:\n",
    "        target_minimum_price_list.append(minimum_price_column[i])\n",
    "        \n",
    "    for time_content_index in range(len(target_time_content_list)):\n",
    "        if time_content_index+ 1== len(target_time_content_list):\n",
    "            break\n",
    "        a_target_time= target_time_content_list[time_content_index]\n",
    "        b_target_time= target_time_content_list[time_content_index+ 1]\n",
    "        \n",
    "        a_target_minumum_price= target_minimum_price_list[time_content_index]\n",
    "        b_target_minumum_price= target_minimum_price_list[time_content_index+ 1]\n",
    "        delta_target_time= convert_2_day_delta(a_target_time, b_target_time)\n",
    "        if delta_target_time== 0:\n",
    "            continue\n",
    "        slope= (float(b_target_minumum_price)- float(a_target_minumum_price))/ delta_target_time\n",
    "    \n",
    "        quartile_slope_index_list.append(a_target_time+ '_'+ b_target_time)\n",
    "        quartile_slope_content_list.append(slope)\n",
    "        \n",
    "    return quartile_slope_index_list, quartile_slope_content_list\n",
    "\n",
    "\n",
    "def get_all_phase_round(data_list, target_quartile_slope, target_quartile_index_list):\n",
    "    time_column= [i[0] for i in data_list]\n",
    "    target_quartile_index_list_c= target_quartile_index_list.copy()\n",
    "    all_phase_round= []\n",
    "    \n",
    "    for target_quartile_index in range(len(target_quartile_index_list_c)):\n",
    "        if target_quartile_index== 0 and len(target_quartile_index_list_c)> 1:\n",
    "           \n",
    "            target_start_time_index= None\n",
    "            phase_round_end_time= target_quartile_index_list_c[target_quartile_index+ 1].split('_')[1]\n",
    "            if not phase_round_end_time in time_column:\n",
    "                continue\n",
    "            tagret_end_time_index= time_column.index(phase_round_end_time)\n",
    "      \n",
    "            \n",
    "        \n",
    "        elif target_quartile_index== len(target_quartile_index_list_c)- 1:\n",
    "\n",
    "            phase_round_start_time= target_quartile_index_list_c[target_quartile_index].split('_')[1]          \n",
    "            if not phase_round_start_time in time_column:\n",
    "                continue\n",
    "            target_start_time_index= time_column.index(phase_round_start_time)           \n",
    "            tagret_end_time_index= None\n",
    "          \n",
    "            \n",
    "        else:\n",
    "            phase_round_start_time= target_quartile_index_list_c[target_quartile_index].split('_')[1]\n",
    "            phase_round_end_time= target_quartile_index_list_c[target_quartile_index+ 1].split('_')[1]\n",
    "            if not phase_round_start_time in time_column or not phase_round_end_time in time_column:\n",
    "                continue\n",
    "            target_start_time_index= time_column.index(phase_round_start_time)\n",
    "            tagret_end_time_index= time_column.index(phase_round_end_time)\n",
    "\n",
    "        \n",
    "       \n",
    "        phase_round_range= data_list.copy()[target_start_time_index: tagret_end_time_index]\n",
    "        \n",
    "        all_phase_round.append(phase_round_range)\n",
    "    return all_phase_round\n",
    "\n",
    "def get_price_delta_column(data_list):\n",
    "    delta_list= []\n",
    "    for index, data in enumerate(data_list):\n",
    "        if index== 0:\n",
    "            delta= 0\n",
    "            delta_list.append(delta)\n",
    "            continue\n",
    "\n",
    "        close_price= data[3]\n",
    "        last_close_price= data_list[index- 1][3]\n",
    "        delta= float(close_price)- float(last_close_price)\n",
    "        delta_list.append(delta)\n",
    "    \n",
    "    return delta_list\n",
    "\n",
    "def get_all_periods_data_to_previous_data_level_with_different_columns(data_list, all_phase_round_range):\n",
    "    # 11: Trading Volume level (2) ; 12: Trading Volume 2 price level (3) ; 13: Top price level (5)\n",
    "    # 14: Bottom price level (6); 15: Transations level (9); 16: Period \n",
    "   \n",
    "    all_data_time_column_list= [i[0] for i in data_list]\n",
    "    target_column_index_list= set([1, 2, 3, 4])\n",
    "    target_column_mean_and_sd_val= []\n",
    "    \n",
    "    \n",
    "    # Target data range\n",
    "    m_range= 10\n",
    "    week_range= 7\n",
    "    \n",
    "    max_range_len= max(m_range, week_range)\n",
    "    for phase_round_range_index, phase_round_range in enumerate(all_phase_round_range):\n",
    "        \n",
    "      \n",
    "        for data_index, data in enumerate(phase_round_range):\n",
    "            if phase_round_range_index== 0 and data_index< max_range_len:\n",
    "                continue\n",
    "                \n",
    "            target_data_index= all_data_time_column_list.index(data[0])\n",
    "           \n",
    "            for target_column_index in target_column_index_list: # Get level in target data column\n",
    "                \n",
    "                target_column=  [float(i[target_column_index]) for i in data_list]\n",
    "                \n",
    "                \n",
    "                target_m_range_begin= target_data_index- m_range\n",
    "                \n",
    "                target_week_range_begin= target_data_index- week_range\n",
    "                    \n",
    "                target_column=  target_column[target_m_range_begin:target_data_index]+   target_column[target_week_range_begin:target_data_index] # Select range\n",
    "     \n",
    "                specific_phase_round_data_mean= statistics.mean(target_column)\n",
    "          \n",
    "                specific_phase_round_data_sd= statistics.stdev(target_column)\n",
    "                \n",
    "                \n",
    "                            \n",
    "                target_data= float(data[target_column_index])\n",
    "              #  specific_phase_round_data_min_c= specific_phase_round_data_min\n",
    "                \n",
    "                data_to_previous_data= ((np.array(target_column)- np.array(target_data))/ specific_phase_round_data_sd).tolist()\n",
    "             \n",
    "               # print ('data_to_previous_data shape', np.array(data_to_previous_data).shape)\n",
    "                \n",
    "                data.append(data_to_previous_data)\n",
    "                \n",
    "            data.append(phase_round_range_index)\n",
    "        \n",
    "        \n",
    "     #   phase_round_range_ori_time_column= [i[0] for i in phase_round_range_ori]\n",
    "     #   new_phase_round_range= []\n",
    "     #   for d in phase_round_range:\n",
    "     #       if d[0] not in phase_round_range_ori_time_column:\n",
    "     #           continue\n",
    "            \n",
    "     #       new_phase_round_range.append(d)\n",
    "     #   phase_round_range= new_phase_round_range\n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "            \n",
    "    return all_phase_round_range, max_range_len\n",
    "\n",
    "\n",
    "def get_period_data_to_previous_data_level_with_different_columns(data_list, all_phase_round_range):\n",
    "    # 11: Trading Volume level (2) ; 12: Trading Volume 2 price level (3) ; 13: Top price level (5)\n",
    "    # 14: Bottom price level (6); 15: Transations level (9); 16: Period \n",
    "    target_column_index_list= set([1, 2, 3, 4])\n",
    "    target_column_mean_and_sd_val= []\n",
    "    \n",
    "    \n",
    "    # Target data range\n",
    "    m_range= 10\n",
    "    week_range= 7\n",
    "    day_range= 7\n",
    "    \n",
    "    phase_round_range_index='n'\n",
    "   \n",
    "    for data_index, data in enumerate(data_list):\n",
    "        for target_column_index in target_column_index_list: # Get level in target data column\n",
    "                target_column=  [float(i[target_column_index]) for i in data_list if i != '']\n",
    "                \n",
    "                if week_range> len(target_column):\n",
    "                    target_week_range_begin= 0\n",
    "                else:\n",
    "                    target_week_range_begin= len(target_column)- week_range\n",
    "                if m_range> len(target_column):\n",
    "                    target_m_range_begin= 0\n",
    "                else:\n",
    "                    target_m_range_begin= len(target_column)- m_range\n",
    "                target_column=  target_column[target_m_range_begin:]+ target_column[target_week_range_begin:]   # Select range\n",
    "               \n",
    "                \n",
    "                specific_phase_round_data_mean= statistics.mean(target_column)\n",
    "              #  specific_phase_round_data_min= min(target_column)\n",
    "                specific_phase_round_data_sd= statistics.stdev(target_column)\n",
    "                \n",
    "                target_data= float(data[target_column_index])\n",
    "             \n",
    "             \n",
    "                    \n",
    "                data_to_previous_data= ((np.array(target_column)- np.array(target_data))/ specific_phase_round_data_sd).tolist()\n",
    "               \n",
    "                \n",
    "                data.append(data_to_previous_data)\n",
    "        data.append(phase_round_range_index)\n",
    "                       \n",
    "            \n",
    "    return data_list\n",
    "\n",
    "def get_sd_level(data_list, target_data_level_column):\n",
    "    sd_level_list= [] \n",
    "    sd_level_tag_column= []\n",
    "    target_data_level_column_c= target_data_level_column.copy()\n",
    "    sd_level= statistics.stdev(target_data_level_column_c)\n",
    "    mean= statistics.mean(target_data_level_column)\n",
    "    for i in range(-9, 10):\n",
    "        r= mean+ sd_level* i\n",
    "        sd_level_list.append(r)\n",
    "        \n",
    "    for target_data in target_data_level_column_c:\n",
    "        \n",
    "        level_tag= None\n",
    "      \n",
    "        for sd_level_index, sd_level in enumerate(sd_level_list):\n",
    "          \n",
    "            if sd_level_index== len(sd_level_list)-1:\n",
    "                if target_data >sd_level:\n",
    "                    level_tag= str(sd_level_index+ 1)\n",
    "                elif target_data>= sd_level_list[sd_level_index-1] and sd_level_list[sd_level_index]<= target_data:\n",
    "                    level_tag= str(sd_level_index)\n",
    "                continue\n",
    "             \n",
    "            if sd_level_index== 0:\n",
    "                if target_data <sd_level_list[sd_level_index]:\n",
    "                    level_tag= str(sd_level_index)\n",
    "                elif target_data >= sd_level_list[sd_level_index] and  target_data<= sd_level_list[sd_level_index+ 1]:\n",
    "                    level_tag= str(sd_level_index+ 1)\n",
    "                continue\n",
    "                \n",
    "            if sd_level_list[sd_level_index+ 1]> target_data and target_data> sd_level_list[sd_level_index] :\n",
    "                level_tag= str(sd_level_index)\n",
    "     \n",
    "        sd_level_tag_column.append(level_tag)  \n",
    "        \n",
    "    return sd_level_list, sd_level_tag_column\n",
    "\n",
    "def get_phase_stage_range(last_phase_round_range):\n",
    "   \n",
    "    minimum_price_list= [float(i[3]) for i in last_phase_round_range] # Lowest price column index= 3\n",
    "    \n",
    "  \n",
    "    # 8 stages\n",
    "    mean= statistics.mean(minimum_price_list)\n",
    "    minimum= min(minimum_price_list)- min(minimum_price_list)* 5/ 100 \n",
    "    maximum= max(minimum_price_list)+ max(minimum_price_list)* 5/ 100\n",
    "    q1, q2, q3= statistics.quantiles(minimum_price_list)\n",
    "   \n",
    "    # 1: minimum- minimum_q1_mean; 2: minimum_q1_mean- q1; 3: q1- q1_q2_mean; 4: q1_q2_mean- q2; \n",
    "    # 5: q2- q2_q3_mean; 6: q2_q3_mean- q3; 7: q3- q3_q4_mean; 8: q3_q4_mean- maximum\n",
    "    minimum_q1_mean= statistics.mean([minimum, q1])\n",
    "    q1_q2_mean= statistics.mean([q1, q2])\n",
    "    q2_q3_mean= statistics.mean([q2, q3])\n",
    "    q3_maximum_mean= statistics.mean([q3, maximum])\n",
    "    \n",
    "    phase_stage_list= [(minimum, minimum_q1_mean), (minimum_q1_mean, q1), (q1, q1_q2_mean), \n",
    "                       (q1_q2_mean, q2), (q2, q2_q3_mean), (q2_q3_mean, q3), \n",
    "                       (q3, q3_maximum_mean), (q3_maximum_mean, maximum)]\n",
    "   \n",
    "                           \n",
    "    return phase_stage_list\n",
    "\n",
    "def tag_phase_stage(target_data_column, phase_stage_range_list):\n",
    "    # If price not in maximum phase stage or minimum stage, check phase round transition\n",
    "    phase_stage_column= []\n",
    "    for target_data in target_data_column:\n",
    "        phase_tag= None\n",
    "        for phase_stage_index in range(len(phase_stage_range_list)):\n",
    "            lower_boundary, upper_boundary= phase_stage_range_list[phase_stage_index]\n",
    "            if target_data >=lower_boundary and target_data<= upper_boundary:\n",
    "                phase_tag= phase_stage_index\n",
    "        if phase_tag== None:\n",
    "            print ('please check phase round transition', target_data)\n",
    "        phase_stage_column.append(phase_tag)\n",
    "    return phase_stage_column\n",
    "\n",
    "def save_train_data(dir, data_list):\n",
    "     with open(dir, 'w', newline='') as csvfile:                       \n",
    "        writer = csv.writer(csvfile)\n",
    "        for data in data_list:\n",
    "            writer.writerow(data)\n",
    "            \n",
    "def get_action(test_data_index, predict_stage, action_history, current_holding):\n",
    "    action= 0\n",
    "    \n",
    "    if test_data_index< 7:\n",
    "        action= 0\n",
    "        \n",
    "    if predict_stage<= 3 and current_holding== 0:\n",
    "        action= 1\n",
    "    elif predict_stage>= 4 and current_holding== 1:\n",
    "        action= -1\n",
    "        \n",
    "        \n",
    "    if action== 1:\n",
    "        current_holding= 1\n",
    "    if action== -1:\n",
    "        current_holding= 0\n",
    "        \n",
    "    action_history.append(action)\n",
    "    \n",
    "    return action, action_history, current_holding\n",
    "\n",
    "def save_action_output(data_list, action_output_dir):\n",
    "   \n",
    "    with open(action_output_dir, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for data in data_list:\n",
    "            writer.writerow([data])    \n",
    "    \n",
    "     \n",
    "def agent_action(test_data_dir, action_output_dir):\n",
    "    \n",
    "    \n",
    "    data_list= get_outline_history_data_for_test(test_data_dir)\n",
    "    action_history=[]\n",
    "    current_holding= 0\n",
    "    for count, i in enumerate(data_list):\n",
    "      #  print (i)\n",
    "        if count< 10:\n",
    "            action_history.append(0)\n",
    "            continue\n",
    "        if count== len(data_list)-1:\n",
    "            break\n",
    "        preprocess_for_test_data(test_data_dir, i)\n",
    "        predict_value= set_and_init_predict()\n",
    "        print (i)\n",
    "        action, action_history, current_holding= get_action(count, predict_value, action_history, current_holding)\n",
    "        print ('action', action, 'action_history', action_history, 'current_holding', current_holding)\n",
    "    print (len(action_history))\n",
    "    save_action_output(action_history, action_output_dir)\n",
    "   \n",
    "       \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfb51e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013-06-07', '55.950001', '56.150002', '55.900002']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[5]\n",
      "['2013-06-07', '55.950001', '56.150002', '55.900002', '56.049999', '56.049999', '9058196']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "['2013-06-10', '56.549999', '56.700001', '56.400002']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[6]\n",
      "['2013-06-10', '56.549999', '56.700001', '56.400002', '56.500000', '56.500000', '3505343']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "['2013-06-11', '56.549999', '56.650002', '56.250000']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[1]\n",
      "['2013-06-11', '56.549999', '56.650002', '56.250000', '56.349998', '56.349998', '4689065']\n",
      "action 1 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] current_holding 1\n",
      "['2013-06-13', '55.900002', '55.900002', '55.000000']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[3]\n",
      "['2013-06-13', '55.900002', '55.900002', '55.000000', '55.000000', '55.000000', '33023834']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] current_holding 1\n",
      "['2013-06-14', '55.150002', '55.150002', '54.700001']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[4]\n",
      "['2013-06-14', '55.150002', '55.150002', '54.700001', '54.799999', '54.799999', '18099232']\n",
      "action -1 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1] current_holding 0\n",
      "['2013-06-17', '54.849998', '55.150002', '54.799999']\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[1]\n",
      "['2013-06-17', '54.849998', '55.150002', '54.799999', '55.150002', '55.150002', '5347377']\n",
      "action 1 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1] current_holding 1\n",
      "['2013-06-18', '55.150002', '55.400002', '54.849998']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[2]\n",
      "['2013-06-18', '55.150002', '55.400002', '54.849998', '55.400002', '55.400002', '8771556']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0] current_holding 1\n",
      "['2013-06-19', '55.500000', '55.500000', '55.150002']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[4]\n",
      "['2013-06-19', '55.500000', '55.500000', '55.150002', '55.299999', '55.299999', '6076740']\n",
      "action -1 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, -1] current_holding 0\n",
      "['2013-06-20', '55.000000', '55.000000', '54.200001']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[2]\n",
      "['2013-06-20', '55.000000', '55.000000', '54.200001', '54.250000', '54.250000', '25487892']\n",
      "action 1 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, -1, 1] current_holding 1\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "#init_time= time.time()\n",
    "#preprocess(os.path.join(os.getcwd(), 'training_data.csv')) #\n",
    "#set_and_init_train()\n",
    "#dur_time= time.time()- init_time\n",
    "#print (dur_time/ 3600)\n",
    "\n",
    "\n",
    "test_data_dir= os.path.join(os.getcwd(), 'testing_data_0.csv')\n",
    "action_output_dir= os.path.join(os.getcwd(), 'output.csv')\n",
    "agent_action(test_data_dir, action_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2b060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--training TRAINING] [--testing TESTING]\n",
      "                             [--output OUTPUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/chois/Library/Jupyter/runtime/kernel-8312e18e-9238-44d6-b5dd-d3317bf2e913.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # You should not modify this part.\n",
    "    import argparse\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--training',\n",
    "                       default='training_data.csv',\n",
    "                       help='input training data file name')\n",
    "    parser.add_argument('--testing',\n",
    "                        default='testing_data.csv',\n",
    "                        help='input testing data file name')\n",
    "    parser.add_argument('--output',\n",
    "                        default='output.csv',\n",
    "                        help='output file name')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # The following part is an example.\n",
    "    # You can modify it at will.\n",
    "    init_time= time.time()\n",
    "    \n",
    "    # For training \n",
    "    preprocess(args.training)\n",
    "    set_and_init_train()\n",
    "\n",
    " \n",
    "    \n",
    "    # For testing\n",
    "    set_and_init_predict()\n",
    "    test_data_dir= args.testing\n",
    "    action_output_dir= args.output\n",
    "    agent_action(test_data_dir, action_output_dir) \n",
    "    print ('All process done.')\n",
    "    duration_time= (time.time()- init_time)/ 60\n",
    "    print (f'All done in {duration_time} min.')\n",
    "    \n",
    "        \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc298b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "!python /Users/chois/trader/StockProfitCalculator/profit_calculator.py /Users/chois/trader/testing_data_0.csv /Users/chois/trader/output.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b682596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.300003000000004\n",
    "# testing_data 2.949996999999996\n",
    "# testing_data_0 0\n",
    "# testing_data_1 2.2881130000000027\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0281860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python trader.py --training training_data_1.csv --testing testing_data_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a65e28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of periods (24,)\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "[4]\n",
      "['2013-11-04', '57.400002', '57.400002', '57.099998', '57.349998', '57.349998', '6647499']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[4]\n",
      "['2013-11-05', '57.250000', '57.349998', '56.750000', '56.799999', '56.799999', '7879372']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[3]\n",
      "['2013-11-06', '56.799999', '56.900002', '56.599998', '56.799999', '56.799999', '4717161']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[3]\n",
      "['2013-11-07', '56.849998', '56.849998', '56.500000', '56.650002', '56.650002', '4901449']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[5]\n",
      "['2013-11-08', '56.599998', '56.650002', '56.349998', '56.450001', '56.450001', '6209803']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[5]\n",
      "['2013-11-11', '56.450001', '56.500000', '56.200001', '56.299999', '56.299999', '8237847']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[4]\n",
      "['2013-11-12', '56.599998', '56.599998', '56.200001', '56.500000', '56.500000', '6243339']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[6]\n",
      "['2013-11-13', '56.250000', '56.299999', '55.700001', '55.750000', '55.750000', '17168796']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[4]\n",
      "['2013-11-14', '56.000000', '56.099998', '55.650002', '55.900002', '55.900002', '15583085']\n",
      "action 0 action_history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] current_holding 0\n",
      "19\n",
      "All process done.\n",
      "All done in 1.6737745761871339 min.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-16 09:42:01.351463: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-16 09:42:03.896359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1638 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-10-16 09:42:06.904076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1638 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-10-16 09:42:18.958449: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "2022-10-16 09:42:19.418828: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025680FFF550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002568E119B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "!python trader.py  --training training_data_0.csv --testing testing_data_0.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8c54e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -rmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "python trader.py  --training training_data_0.csv --testing testing_data.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
